{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "510a5072-054c-4dce-a061-2ca3dc4301f3",
   "metadata": {},
   "source": [
    "# Training a Modified LeNet CNN for Breast Cancer Image Classification and deploying on FPGA target, using HLS4ML.\n",
    "## Vaggelis Ananiadis, Supervisor: Prof. Karakonstantis G.\n",
    "#### ECE-284 - Processor Design\n",
    "#### Training code was modified from https://github.com/m3mentomor1/Breast-Cancer-Image-Classification-with-DenseNet121"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f0a6ab-2c3c-4785-a0ed-ce0fa03f3c74",
   "metadata": {},
   "source": [
    "## Optimized Version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcc4023-3c46-4f37-8f90-e9066d8bb131",
   "metadata": {},
   "source": [
    "Modified LeNet architecture:\n",
    "Balasubramaniam S, Velmurugan Y, Jaganathan D, Dhanasekaran S. A Modified LeNet CNN for Breast Cancer Diagnosis in Ultrasound Images. Diagnostics (Basel). 2023;13(17):2746. Published 2023 Aug 24. doi:10.3390/diagnostics13172746"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e729aef-c37d-446c-8619-35a9895f79f5",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e3234216-9bad-459a-9b5d-0367cc6a7880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd             # Pandas\n",
    "import numpy as np              # NumPy\n",
    "import matplotlib.pyplot as plt # Matplotlib\n",
    "import seaborn as sns           # Seaborn\n",
    "from PIL import Image           # Pillow\n",
    "import pathlib\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import models, optimizers, layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "\n",
    "# # Keras\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# hls4ml\n",
    "import hls4ml\n",
    "from hls4ml.model.profiling import numerical, get_ymodel_keras\n",
    "\n",
    "# Custom methods\n",
    "from callbacks import all_callbacks # Custom callbacks method from hls4ml tutorial\n",
    "# from custom_plotting import makeRoc # Custom plotting method from hls4ml tutorial\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "%matplotlib inline\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "os.environ['PATH'] = '/tools/Xilinx/Vitis_HLS/2023.2/bin:' + os.environ['PATH'] # Vitis Path\n",
    "os.environ['PATH'] = '/tools/Xilinx/Vivado/2023.2/bin:' + os.environ['PATH'] # Vivado Path\n",
    "\n",
    "model_dir = \"opt_model/\" #  Directory to generate tf model.\n",
    "hls4ml_dir = 'hls4ml_dir' # Directory to generate firmware.\n",
    "# fpga_target = 'xc7z020clg484-1' # FPGA target (for zedboard: xc7z020clg484-1).\n",
    "fpga_target = 'XCZU3EG-SBVA484-1-E' # a Zynq UltraScale+ MPSoC\n",
    "backend = 'Vitis' # Backend to be used (Vitis, Vivado, VivadoAccelerator).\n",
    "batch_size = 32 # Set the batch size for training.\n",
    "image_size = (16, 16) # Define the target image size for preprocessing.\n",
    "num_channels = 1 # Specify the number of color channels in the images (3 for RGB).\n",
    "image_shape = (image_size[0], image_size[1], num_channels) # Create the image shape tuple based on the specified size and channels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b587bdbb-d230-4e88-bda7-11a3d1dd7e3d",
   "metadata": {},
   "source": [
    "## 2. Load Dataset.\n",
    "#### Check dataset_prep.ipynb for data preprocessing.\n",
    "#### Training dataset was taken from: \n",
    "https://www.kaggle.com/datasets/aryashah2k/breast-ultrasound-images-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68d9fbd7-1fc5-4d43-9880-8c2a5cd6e346",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'dataset'\n",
    "X_train_val = np.load(os.path.join(OUTPUT_PATH, 'X_train_16.npy'))\n",
    "X_val       = np.load(os.path.join(OUTPUT_PATH, 'X_val_16.npy'))\n",
    "X_test      = np.load(os.path.join(OUTPUT_PATH, 'X_test_16.npy'))\n",
    "\n",
    "y_train_val = np.load(os.path.join(OUTPUT_PATH, 'y_train.npy'))\n",
    "y_val       = np.load(os.path.join(OUTPUT_PATH, 'y_val.npy'))\n",
    "y_test      = np.load(os.path.join(OUTPUT_PATH, 'y_test.npy'))\n",
    "classes     = np.load(os.path.join(OUTPUT_PATH, 'classes.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052ab3a0-b174-4690-bb59-18943244d388",
   "metadata": {},
   "source": [
    "## 3. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7268b6-bb5d-45f0-b3f9-01b8c1ab6c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "num_classes = 3\n",
    "dropout_rate = 0.2\n",
    "learning_rate = 0.01\n",
    "input_shape = image_shape\n",
    "\n",
    "inputs = layers.Input(shape=input_shape, name='input1')\n",
    "\n",
    "x = layers.Conv2D(4, kernel_size=3, strides=1, padding='same', activation='relu')(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(dropout_rate)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "x = layers.Conv2D(8, kernel_size=2, strides=2, padding='same', activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(dropout_rate)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "x = layers.Conv2D(16, kernel_size=4, strides=2, padding='same', activation='relu')(x)\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c939e875-4296-4e6b-8aef-0d4d6bbb8852",
   "metadata": {},
   "source": [
    "## 4. Train Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca81011e-20fe-4e65-9ae9-7d7aad7446ac",
   "metadata": {},
   "source": [
    "### Train sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "faa9da91-15ea-443d-9aa1-b2d25e74f74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "\n",
    "pruning_params = {\n",
    "    \"pruning_schedule\": pruning_schedule.ConstantSparsity(\n",
    "        target_sparsity=0.75,     \n",
    "        begin_step=31 * 5,        # Start after 5 epochs\n",
    "        frequency=31              # Prune every epoch\n",
    "    )\n",
    "}\n",
    "model = prune.prune_low_magnitude(model, **pruning_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad45320-302f-416e-8593-e76b51bf1c78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "train = True\n",
    "if train:\n",
    "    adam = Adam(lr=0.001)\n",
    "\n",
    "    # loss_fn = CategoricalCrossentropy(label_smoothing=0.1)\n",
    "    loss_fn = CategoricalCrossentropy()\n",
    "    model.compile(optimizer=adam, loss=loss_fn, metrics=['accuracy'])\n",
    "    callbacks = all_callbacks(\n",
    "        stop_patience=1000,\n",
    "        lr_factor=0.5,\n",
    "        lr_patience=10,\n",
    "        lr_epsilon=0.000001,\n",
    "        lr_cooldown=2,\n",
    "        lr_minimum=0.0000001,\n",
    "        outputDir=model_dir,\n",
    "    )\n",
    "    callbacks.callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
    "    model.fit(\n",
    "        X_train_val,\n",
    "        y_train_val,\n",
    "        batch_size=batch_size,\n",
    "        epochs=50,\n",
    "        validation_data=(X_val, y_val),\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks.callbacks,\n",
    "    )\n",
    "    # Save the model again but with the pruning 'stripped' to use the regular layer types\n",
    "    model = strip_pruning(model)\n",
    "    model.save(model_dir + 'KERAS_check_best_model.h5')\n",
    "else:\n",
    "    from tensorflow.keras.models import load_model\n",
    "    from qkeras.utils import _add_supported_quantized_objects\n",
    "\n",
    "    co = {}\n",
    "    _add_supported_quantized_objects(co)\n",
    "    model = load_model(model_dir + 'KERAS_check_best_model.h5', custom_objects=co, compile=False)\n",
    "    \n",
    "# Manually recompile\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss=CategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "y_keras = model.predict(X_test)\n",
    "print(\"\\n\\nAccuracy: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_keras, axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef670e0-bf57-4df3-b533-cdb41c5dab63",
   "metadata": {},
   "source": [
    "## 5. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573b6a5c-e9aa-4e56-9f57-a3fdb7db3439",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "y_keras = model.predict(X_test)\n",
    "end_time = time.time()\n",
    "print(\"Accuracy: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_keras, axis=1))))\n",
    "print(f'Inference time: {round((end_time - start_time)*1000, 3)}ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2182aac9-6cf7-4156-bcc0-0c9106ca9778",
   "metadata": {},
   "source": [
    "## 6. Convert to FPGA firmware w/ hls4ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eb2ea4-c8ce-4390-ad73-ccdc2b5a2c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = hls4ml.utils.config_from_keras_model(model, \n",
    "                                              granularity='model', \n",
    "                                              default_reuse_factor=16)\n",
    "                                              # default_precision='fixed<12,2>')\n",
    "\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "    model,\n",
    "    hls_config=config, \n",
    "    backend=backend, \n",
    "    output_dir=model_dir + hls4ml_dir, \n",
    "    part=fpga_target,\n",
    "    io_type='io_stream')\n",
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bd6e78-6bf3-4ac9-811f-1d40d9aee148",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hls_model.build(csim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b905c7a4-293f-49bb-b0b7-12b5438cad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.ascontiguousarray(X_test)\n",
    "y_hls = hls_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b6130cde-ea95-43aa-a330-1ea71bab28a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras  Accuracy: 0.7721518987341772\n",
      "hls4ml Accuracy: 0.6708860759493671\n"
     ]
    }
   ],
   "source": [
    "print(\"Keras  Accuracy: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_keras, axis=1))))\n",
    "print(\"hls4ml Accuracy: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_hls, axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdddb216-53c9-4e15-8b92-8864ebe9dd12",
   "metadata": {},
   "source": [
    "## 7. Check Vitis HLS reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ec7b8-a4d0-463a-8bbe-78c6efec10c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hls4ml.report.read_vivado_report(model_dir + hls4ml_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
